# Apache Airflow - orchestration of obstetric pipeline (DAGs)
# For full deployment use Helm: apache-airflow/airflow with KubernetesExecutor
# This is a minimal deployment for dev; production should use Helm chart.
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-webserver
  namespace: obs-prod
  labels:
    app: airflow
    component: webserver
spec:
  replicas: 1
  selector:
    matchLabels:
      app: airflow
      component: webserver
  template:
    metadata:
      labels:
        app: airflow
        component: webserver
    spec:
      containers:
        - name: webserver
          image: apache/airflow:2.9.0-python3.11
          env:
            - name: AIRFLOW__CORE__EXECUTOR
              value: "LocalExecutor"
            - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
              valueFrom:
                secretKeyRef:
                  name: airflow-db-credentials
                  key: connection
            - name: AIRFLOW__WEBSERVER__EXPOSE_CONFIG
              value: "false"
            - name: AIRFLOW__API__AUTH_BACKENDS
              value: "airflow.api.auth.backend.basic_auth"
          ports:
            - containerPort: 8080
              name: airflow
          volumeMounts:
            - name: dags
              mountPath: /opt/airflow/dags
          resources:
            requests:
              memory: "512Mi"
              cpu: "250m"
            limits:
              memory: "2Gi"
              cpu: "1000m"
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 60
            periodSeconds: 30
      volumes:
        - name: dags
          configMap:
            name: airflow-dags
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: airflow-dags
  namespace: obs-prod
data:
  obstetric_pipeline.py: |
    # Placeholder - full DAG in orchestrator/dags/
    from airflow import DAG
    from airflow.operators.python import PythonOperator
    from datetime import datetime
    with DAG("obstetric_intelligence_pipeline", start_date=datetime(2025, 1, 1), schedule=None, catchup=False) as dag:
        PythonOperator(task_id="placeholder", python_callable=lambda: None)
---
apiVersion: v1
kind: Service
metadata:
  name: airflow-webserver
  namespace: obs-prod
spec:
  ports:
    - port: 8080
      targetPort: 8080
      name: airflow
  selector:
    app: airflow
    component: webserver
---
apiVersion: v1
kind: Secret
metadata:
  name: airflow-db-credentials
  namespace: obs-prod
type: Opaque
stringData:
  connection: "postgresql+psycopg2://postgres:change-me-use-external-secrets@postgresql.obs-prod.svc.cluster.local:5432/airflow"
